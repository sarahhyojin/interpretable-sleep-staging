{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9304fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
    "\n",
    "import os\n",
    "os.chdir(f'./Transformer-Explainability')\n",
    "\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6aa575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import PIL\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.ViT.ViT_LRP import sleep_base_patch16_224 as vit_SLEEP\n",
    "from baselines.ViT.ViT_explanation_generator import LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ViT pretrained with vit_SLEEP\n",
    "model_path = \"/tf/data_AIoT1/ViT_models/ViT-full-2023-12-09.pth\"\n",
    "model = vit_SLEEP(pretrained=True, checkpoint_path = model_path).cuda()\n",
    "model.eval()\n",
    "attribution_generator = LRP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = \"/tf/data_AIoT1/psg_image/labels/test_1209.txt\"\n",
    "os.path.exists(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/tf/data_AIoT1/psg_image/full_test_1116/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test_labels\n",
    "df_file_list = pd.read_csv(test_labels, sep=\"\\t\", header=None)\n",
    "files = df_file_list[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create patient_dict\n",
    "patient_dict = {} # number of vectors that is from the same patient\n",
    "prev_patient = None\n",
    "\n",
    "for file in files:\n",
    "    patient = file[0:16]\n",
    "    if patient == prev_patient:\n",
    "        patient_dict[patient] += 1\n",
    "    else:\n",
    "        patient_dict[patient] = 1\n",
    "    prev_patient = patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import PIL\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "class IntraEpochDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        df = pd.read_csv(annotations_file, sep=\"\\t\", header=None)\n",
    "        self.labels = dict(zip(df[0], df[1])) # file_name will be the key labels will be the value\n",
    "        self.image_filenames = list(self.labels.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.img_dir, image_name)\n",
    "\n",
    "        image = torch.from_numpy(np.load(image_path).astype(np.float32))\n",
    "        label = int(self.labels[image_name])\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = IntraEpochDataset(test_labels, img_path)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_img = images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_lbl = labels[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7674d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0:'Wake', 1:'N1', 2:'N2', 3:'N3', 4:'REM'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ex_img.permute(1, 2, 0))\n",
    "plt.title(labels_map[ex_lbl])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualization(original_image, class_index=None):\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    # print(\"transformer_attribution before reshaping\", transformer_attribution)\n",
    "    # print(\"sum up the attribution\", transformer_attribution.sum())\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
    "    # interpolate -> upsampling\n",
    "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.reshape(224, 224).data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    # print(\"after normalization\", transformer_attribution)\n",
    "    # print(\"max and min\", transformer_attribution.max(), transformer_attribution.min())\n",
    "#     if use_thresholding:\n",
    "#         transformer_attribution = transformer_attribution * 255\n",
    "#         transformer_attribution = transformer_attribution.astype(np.uint8)\n",
    "#         ret, transformer_attribution = cv2.threshold(transformer_attribution, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "#         transformer_attribution[transformer_attribution == 255] = 1\n",
    "\n",
    "    image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    \n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return vis\n",
    "\n",
    "def print_top_classes(predictions, **kwargs):    \n",
    "    # Print Top-5 predictions\n",
    "    prob = torch.softmax(predictions, dim=1)\n",
    "    # print(\"softmax values\", prob)\n",
    "    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()\n",
    "    max_str_len = 4\n",
    "    class_names = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
    "\n",
    "    \n",
    "    print('Top 5 classes:')\n",
    "    for cls_idx in class_indices:\n",
    "        output_string = '\\t{} : {}'.format(cls_idx, class_names[cls_idx])\n",
    "        output_string += ' ' * (max_str_len - len(class_names[cls_idx])) + '\\t\\t'\n",
    "        output_string += 'value = {:.3f}\\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])\n",
    "        print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(output):\n",
    "    labels_map = {0:'Wake', 1:'N1', 2:'N2', 3:'N3', 4:'REM'}\n",
    "    \n",
    "    prob = torch.softmax(output, dim=1)\n",
    "    _, pred = prob.max(dim=1)\n",
    "    pred = labels_map[pred.item()]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523fbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmap from mask on image\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show original image and ground truth label\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(f\"Ground Truth: {labels_map[ex_lbl]}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create output and predictions\n",
    "output = model(image.unsqueeze(0).cuda())\n",
    "print_top_classes(output)\n",
    "pred = get_predictions(output)\n",
    "im = generate_visualization(image)\n",
    "plt.imshow(im)\n",
    "\n",
    "# show prediction and explanations\n",
    "plt.title(f\"Prediction: {pred}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inidivdual(img_path, labels):\n",
    "    image = torch.from_numpy(np.load(img_path).astype(np.float32))\n",
    "    ex_lbl = labels[ex_path.split('/')[-1]]\n",
    "    \n",
    "    # show original image and ground truth label\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.title(f\"Ground Truth: {labels_map[ex_lbl]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # create output and predictions\n",
    "    output = model(image.unsqueeze(0).cuda())\n",
    "    print_top_classes(output)\n",
    "    pred = get_predictions(output)\n",
    "    im = generate_visualization(image)\n",
    "    plt.imshow(im)\n",
    "\n",
    "    # show prediction and explanations\n",
    "    plt.title(f\"Prediction: {pred}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_inidivdual('/tf/data_AIoT1/psg_image/full_test_1116/A2020-NX-01-0179_0279.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6380b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntraEpochDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        df = pd.read_csv(annotations_file, sep=\"\\t\", header=None)\n",
    "        self.labels = dict(zip(df[0], df[1])) # file_name will be the key labels will be the value\n",
    "\n",
    "        self.image_paths = []\n",
    "        for path in os.listdir(self.img_dir):\n",
    "            self.image_paths.append(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_paths[idx]\n",
    "        image_path = os.path.join(self.img_dir, image_name)\n",
    "\n",
    "        image = torch.from_numpy(np.load(image_path).astype(np.float32))\n",
    "        label = int(self.labels[image_name])\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = IntraEpochDataset(test_label_path, test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8107e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total test images: ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=80, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe256fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_dataloader))\n",
    "images[0].shape\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0:'Wake', 1:'N1', 2:'N2', 3:'N3', 4:'REM'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a05785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = torch.randint(len(images), size=(1,)).item()\n",
    "img, label = images[sample_idx], labels[sample_idx].item()\n",
    "plt.title(labels_map[label])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.permute((1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_heatmap(images, labels):\n",
    "    # image = torch.from_numpy(np.load(ex_path).astype(np.float32)/255.0)\n",
    "    for image, label in zip(images, labels):\n",
    "        # show original image and ground truth label\n",
    "        plt.imshow(image.permute(1, 2, 0))\n",
    "        label = label.tolist()\n",
    "        plt.title(f\"Ground Truth: {labels_map[label]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # create output and predictions\n",
    "        output = model(image.unsqueeze(0).cuda())\n",
    "        # print_top_classes(output)\n",
    "        pred = get_predictions(output)\n",
    "        im = generate_visualization(image)\n",
    "        plt.imshow(im)\n",
    "\n",
    "        # show prediction and explanations\n",
    "        plt.title(f\"Prediction: {pred}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f575b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_image_heatmap(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_image_heatmap(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06265c63",
   "metadata": {},
   "source": [
    "# Visualize only correct guesses & aggregate\n",
    "- see the events of that epoch \n",
    "- event labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_thresholding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_heatmaps = {class_index: None for class_index in range(5)}\n",
    "class_num = {class_index: None for class_index in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_attribution(original_image, class_index=None):\n",
    "    # generate mask\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
    "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.reshape(224, 224).data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    \n",
    "    if use_thresholding:\n",
    "        transformer_attribution = transformer_attribution * 255\n",
    "        transformer_attribution = transformer_attribution.astype(np.uint8)\n",
    "        ret, transformer_attribution = cv2.threshold(transformer_attribution, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        transformer_attribution[transformer_attribution == 255] = 1\n",
    "        \n",
    "    return transformer_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "num_samples = 10000\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for inputs, labels in tqdm(test_dataloader):\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.data.cpu().numpy()\n",
    "\n",
    "    # Feed Network and get predictions\n",
    "    output = model(inputs)\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output)  # Save Prediction\n",
    "    y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    # Calculate heatmap for each correct prediction in this batch\n",
    "    correct_mask = (output == labels)\n",
    "    # print(output, labels)\n",
    "    # print(correct_mask)\n",
    "    correct_indices = correct_mask.nonzero()[0]\n",
    "    # print(correct_indices)\n",
    "    for idx in correct_indices:\n",
    "        # print(idx)\n",
    "        i = idx.item()  # Extract the index as a scalar\n",
    "        heatmap = transformer_attribution(inputs[i])\n",
    "        lbl = labels[i]\n",
    "        \n",
    "        # save until 10000 labels for each class\n",
    "        if class_heatmaps[lbl] is None:\n",
    "            class_heatmaps[lbl] = heatmap / num_samples\n",
    "            class_num[lbl] = 1\n",
    "        elif class_num[lbl] < num_samples:\n",
    "            class_heatmaps[lbl] += (heatmap / num_samples)\n",
    "            class_num[lbl] += 1\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant for classes\n",
    "classes = ('Wake', 'N1', 'N2', 'N3', 'REM')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1).reshape((5, 1)), index = [i for i in classes],\n",
    "                    columns = [i for i in classes])\n",
    "plt.figure(figsize = (5,4))\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
